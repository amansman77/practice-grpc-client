# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: grpc_service_v2.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from tritongrpcclient import model_config_pb2 as model__config__pb2


DESCRIPTOR = _descriptor.FileDescriptor(
  name='grpc_service_v2.proto',
  package='nvidia.inferenceserver',
  syntax='proto3',
  serialized_options=None,
  serialized_pb=_b('\n\x15grpc_service_v2.proto\x12\x16nvidia.inferenceserver\x1a\x12model_config.proto\"\x13\n\x11ServerLiveRequest\"\"\n\x12ServerLiveResponse\x12\x0c\n\x04live\x18\x01 \x01(\x08\"\x14\n\x12ServerReadyRequest\"$\n\x13ServerReadyResponse\x12\r\n\x05ready\x18\x01 \x01(\x08\"2\n\x11ModelReadyRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\"#\n\x12ModelReadyResponse\x12\r\n\x05ready\x18\x01 \x01(\x08\"\x17\n\x15ServerMetadataRequest\"K\n\x16ServerMetadataResponse\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\x12\x12\n\nextensions\x18\x03 \x03(\t\"5\n\x14ModelMetadataRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\"\xa7\x02\n\x15ModelMetadataResponse\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08versions\x18\x02 \x03(\t\x12\x10\n\x08platform\x18\x03 \x01(\t\x12L\n\x06inputs\x18\x04 \x03(\x0b\x32<.nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata\x12M\n\x07outputs\x18\x05 \x03(\x0b\x32<.nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata\x1a?\n\x0eTensorMetadata\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\"i\n\x0eInferParameter\x12\x14\n\nbool_param\x18\x01 \x01(\x08H\x00\x12\x15\n\x0bint64_param\x18\x02 \x01(\x03H\x00\x12\x16\n\x0cstring_param\x18\x03 \x01(\tH\x00\x42\x12\n\x10parameter_choice\"\xe5\x01\n\x13InferTensorContents\x12\x14\n\x0craw_contents\x18\x01 \x01(\x0c\x12\x15\n\rbool_contents\x18\x02 \x03(\x08\x12\x14\n\x0cint_contents\x18\x03 \x03(\x05\x12\x16\n\x0eint64_contents\x18\x04 \x03(\x03\x12\x15\n\ruint_contents\x18\x05 \x03(\r\x12\x17\n\x0fuint64_contents\x18\x06 \x03(\x04\x12\x15\n\rfp32_contents\x18\x07 \x03(\x02\x12\x15\n\rfp64_contents\x18\x08 \x03(\x01\x12\x15\n\rbyte_contents\x18\t \x03(\x0c\"\xc7\x07\n\x11ModelInferRequest\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\t\x12\n\n\x02id\x18\x03 \x01(\t\x12M\n\nparameters\x18\x04 \x03(\x0b\x32\x39.nvidia.inferenceserver.ModelInferRequest.ParametersEntry\x12J\n\x06inputs\x18\x05 \x03(\x0b\x32:.nvidia.inferenceserver.ModelInferRequest.InferInputTensor\x12U\n\x07outputs\x18\x06 \x03(\x0b\x32\x44.nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor\x1a\xbb\x02\n\x10InferInputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12^\n\nparameters\x18\x04 \x03(\x0b\x32J.nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry\x12=\n\x08\x63ontents\x18\x05 \x01(\x0b\x32+.nvidia.inferenceserver.InferTensorContents\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\x1a\xef\x01\n\x1aInferRequestedOutputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12h\n\nparameters\x18\x02 \x03(\x0b\x32T.nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\"\xc9\x03\n\x12ModelInferResponse\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\t\x12\n\n\x02id\x18\x03 \x01(\t\x12N\n\nparameters\x18\x04 \x03(\x0b\x32:.nvidia.inferenceserver.ModelInferResponse.ParametersEntry\x12M\n\x07outputs\x18\x05 \x03(\x0b\x32<.nvidia.inferenceserver.ModelInferResponse.InferOutputTensor\x1a\x81\x01\n\x11InferOutputTensor\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08\x64\x61tatype\x18\x02 \x01(\t\x12\r\n\x05shape\x18\x03 \x03(\x03\x12=\n\x08\x63ontents\x18\x04 \x01(\x0b\x32+.nvidia.inferenceserver.InferTensorContents\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.InferParameter:\x02\x38\x01\"u\n\x18ModelStreamInferResponse\x12\x15\n\rerror_message\x18\x01 \x01(\t\x12\x42\n\x0einfer_response\x18\x02 \x01(\x0b\x32*.nvidia.inferenceserver.ModelInferResponse\"3\n\x12ModelConfigRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\"J\n\x13ModelConfigResponse\x12\x33\n\x06\x63onfig\x18\x01 \x01(\x0b\x32#.nvidia.inferenceserver.ModelConfig\"7\n\x16ModelStatisticsRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\".\n\x11StatisticDuration\x12\r\n\x05\x63ount\x18\x01 \x01(\x04\x12\n\n\x02ns\x18\x02 \x01(\x04\"\x87\x03\n\x0fInferStatistics\x12:\n\x07success\x18\x01 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12\x37\n\x04\x66\x61il\x18\x02 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12\x38\n\x05queue\x18\x03 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12@\n\rcompute_input\x18\x04 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12@\n\rcompute_infer\x18\x05 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\x12\x41\n\x0e\x63ompute_output\x18\x06 \x01(\x0b\x32).nvidia.inferenceserver.StatisticDuration\"r\n\x0fModelStatistics\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\t\x12@\n\x0finference_stats\x18\x03 \x01(\x0b\x32\'.nvidia.inferenceserver.InferStatistics\"W\n\x17ModelStatisticsResponse\x12<\n\x0bmodel_stats\x18\x01 \x03(\x0b\x32\'.nvidia.inferenceserver.ModelStatistics\"1\n\x16RepositoryIndexRequest\x12\x17\n\x0frepository_name\x18\x01 \x01(\t\"\x81\x01\n\x17RepositoryIndexResponse\x12J\n\x06models\x18\x01 \x03(\x0b\x32:.nvidia.inferenceserver.RepositoryIndexResponse.ModelIndex\x1a\x1a\n\nModelIndex\x12\x0c\n\x04name\x18\x01 \x01(\t\"I\n\x1aRepositoryModelLoadRequest\x12\x17\n\x0frepository_name\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\"\x1d\n\x1bRepositoryModelLoadResponse\"K\n\x1cRepositoryModelUnloadRequest\x12\x17\n\x0frepository_name\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\"\x1f\n\x1dRepositoryModelUnloadResponse\"/\n\x1fSystemSharedMemoryStatusRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"\xbf\x02\n SystemSharedMemoryStatusResponse\x12V\n\x07regions\x18\x01 \x03(\x0b\x32\x45.nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionsEntry\x1aL\n\x0cRegionStatus\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\t\x12\x0e\n\x06offset\x18\x03 \x01(\x04\x12\x11\n\tbyte_size\x18\x04 \x01(\x04\x1au\n\x0cRegionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12T\n\x05value\x18\x02 \x01(\x0b\x32\x45.nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus:\x02\x38\x01\"a\n!SystemSharedMemoryRegisterRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0b\n\x03key\x18\x02 \x01(\t\x12\x0e\n\x06offset\x18\x03 \x01(\x04\x12\x11\n\tbyte_size\x18\x04 \x01(\x04\"$\n\"SystemSharedMemoryRegisterResponse\"3\n#SystemSharedMemoryUnregisterRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"&\n$SystemSharedMemoryUnregisterResponse\"-\n\x1d\x43udaSharedMemoryStatusRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"\xaf\x02\n\x1e\x43udaSharedMemoryStatusResponse\x12T\n\x07regions\x18\x01 \x03(\x0b\x32\x43.nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionsEntry\x1a\x42\n\x0cRegionStatus\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x11\n\tdevice_id\x18\x02 \x01(\x04\x12\x11\n\tbyte_size\x18\x03 \x01(\x04\x1as\n\x0cRegionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12R\n\x05value\x18\x02 \x01(\x0b\x32\x43.nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionStatus:\x02\x38\x01\"i\n\x1f\x43udaSharedMemoryRegisterRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x12\n\nraw_handle\x18\x02 \x01(\x0c\x12\x11\n\tdevice_id\x18\x03 \x01(\x03\x12\x11\n\tbyte_size\x18\x04 \x01(\x04\"\"\n CudaSharedMemoryRegisterResponse\"1\n!CudaSharedMemoryUnregisterRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\"$\n\"CudaSharedMemoryUnregisterResponse2\xef\x11\n\x14GRPCInferenceService\x12\x65\n\nServerLive\x12).nvidia.inferenceserver.ServerLiveRequest\x1a*.nvidia.inferenceserver.ServerLiveResponse\"\x00\x12h\n\x0bServerReady\x12*.nvidia.inferenceserver.ServerReadyRequest\x1a+.nvidia.inferenceserver.ServerReadyResponse\"\x00\x12\x65\n\nModelReady\x12).nvidia.inferenceserver.ModelReadyRequest\x1a*.nvidia.inferenceserver.ModelReadyResponse\"\x00\x12q\n\x0eServerMetadata\x12-.nvidia.inferenceserver.ServerMetadataRequest\x1a..nvidia.inferenceserver.ServerMetadataResponse\"\x00\x12n\n\rModelMetadata\x12,.nvidia.inferenceserver.ModelMetadataRequest\x1a-.nvidia.inferenceserver.ModelMetadataResponse\"\x00\x12\x65\n\nModelInfer\x12).nvidia.inferenceserver.ModelInferRequest\x1a*.nvidia.inferenceserver.ModelInferResponse\"\x00\x12u\n\x10ModelStreamInfer\x12).nvidia.inferenceserver.ModelInferRequest\x1a\x30.nvidia.inferenceserver.ModelStreamInferResponse\"\x00(\x01\x30\x01\x12h\n\x0bModelConfig\x12*.nvidia.inferenceserver.ModelConfigRequest\x1a+.nvidia.inferenceserver.ModelConfigResponse\"\x00\x12t\n\x0fModelStatistics\x12..nvidia.inferenceserver.ModelStatisticsRequest\x1a/.nvidia.inferenceserver.ModelStatisticsResponse\"\x00\x12t\n\x0fRepositoryIndex\x12..nvidia.inferenceserver.RepositoryIndexRequest\x1a/.nvidia.inferenceserver.RepositoryIndexResponse\"\x00\x12\x80\x01\n\x13RepositoryModelLoad\x12\x32.nvidia.inferenceserver.RepositoryModelLoadRequest\x1a\x33.nvidia.inferenceserver.RepositoryModelLoadResponse\"\x00\x12\x86\x01\n\x15RepositoryModelUnload\x12\x34.nvidia.inferenceserver.RepositoryModelUnloadRequest\x1a\x35.nvidia.inferenceserver.RepositoryModelUnloadResponse\"\x00\x12\x8f\x01\n\x18SystemSharedMemoryStatus\x12\x37.nvidia.inferenceserver.SystemSharedMemoryStatusRequest\x1a\x38.nvidia.inferenceserver.SystemSharedMemoryStatusResponse\"\x00\x12\x95\x01\n\x1aSystemSharedMemoryRegister\x12\x39.nvidia.inferenceserver.SystemSharedMemoryRegisterRequest\x1a:.nvidia.inferenceserver.SystemSharedMemoryRegisterResponse\"\x00\x12\x9b\x01\n\x1cSystemSharedMemoryUnregister\x12;.nvidia.inferenceserver.SystemSharedMemoryUnregisterRequest\x1a<.nvidia.inferenceserver.SystemSharedMemoryUnregisterResponse\"\x00\x12\x89\x01\n\x16\x43udaSharedMemoryStatus\x12\x35.nvidia.inferenceserver.CudaSharedMemoryStatusRequest\x1a\x36.nvidia.inferenceserver.CudaSharedMemoryStatusResponse\"\x00\x12\x8f\x01\n\x18\x43udaSharedMemoryRegister\x12\x37.nvidia.inferenceserver.CudaSharedMemoryRegisterRequest\x1a\x38.nvidia.inferenceserver.CudaSharedMemoryRegisterResponse\"\x00\x12\x95\x01\n\x1a\x43udaSharedMemoryUnregister\x12\x39.nvidia.inferenceserver.CudaSharedMemoryUnregisterRequest\x1a:.nvidia.inferenceserver.CudaSharedMemoryUnregisterResponse\"\x00\x62\x06proto3')
  ,
  dependencies=[model__config__pb2.DESCRIPTOR,])




_SERVERLIVEREQUEST = _descriptor.Descriptor(
  name='ServerLiveRequest',
  full_name='nvidia.inferenceserver.ServerLiveRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=69,
  serialized_end=88,
)


_SERVERLIVERESPONSE = _descriptor.Descriptor(
  name='ServerLiveResponse',
  full_name='nvidia.inferenceserver.ServerLiveResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='live', full_name='nvidia.inferenceserver.ServerLiveResponse.live', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=90,
  serialized_end=124,
)


_SERVERREADYREQUEST = _descriptor.Descriptor(
  name='ServerReadyRequest',
  full_name='nvidia.inferenceserver.ServerReadyRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=126,
  serialized_end=146,
)


_SERVERREADYRESPONSE = _descriptor.Descriptor(
  name='ServerReadyResponse',
  full_name='nvidia.inferenceserver.ServerReadyResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='ready', full_name='nvidia.inferenceserver.ServerReadyResponse.ready', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=148,
  serialized_end=184,
)


_MODELREADYREQUEST = _descriptor.Descriptor(
  name='ModelReadyRequest',
  full_name='nvidia.inferenceserver.ModelReadyRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelReadyRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelReadyRequest.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=186,
  serialized_end=236,
)


_MODELREADYRESPONSE = _descriptor.Descriptor(
  name='ModelReadyResponse',
  full_name='nvidia.inferenceserver.ModelReadyResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='ready', full_name='nvidia.inferenceserver.ModelReadyResponse.ready', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=238,
  serialized_end=273,
)


_SERVERMETADATAREQUEST = _descriptor.Descriptor(
  name='ServerMetadataRequest',
  full_name='nvidia.inferenceserver.ServerMetadataRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=275,
  serialized_end=298,
)


_SERVERMETADATARESPONSE = _descriptor.Descriptor(
  name='ServerMetadataResponse',
  full_name='nvidia.inferenceserver.ServerMetadataResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ServerMetadataResponse.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ServerMetadataResponse.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='extensions', full_name='nvidia.inferenceserver.ServerMetadataResponse.extensions', index=2,
      number=3, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=300,
  serialized_end=375,
)


_MODELMETADATAREQUEST = _descriptor.Descriptor(
  name='ModelMetadataRequest',
  full_name='nvidia.inferenceserver.ModelMetadataRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelMetadataRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelMetadataRequest.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=377,
  serialized_end=430,
)


_MODELMETADATARESPONSE_TENSORMETADATA = _descriptor.Descriptor(
  name='TensorMetadata',
  full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='datatype', full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata.datatype', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata.shape', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=665,
  serialized_end=728,
)

_MODELMETADATARESPONSE = _descriptor.Descriptor(
  name='ModelMetadataResponse',
  full_name='nvidia.inferenceserver.ModelMetadataResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelMetadataResponse.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='versions', full_name='nvidia.inferenceserver.ModelMetadataResponse.versions', index=1,
      number=2, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='platform', full_name='nvidia.inferenceserver.ModelMetadataResponse.platform', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='inputs', full_name='nvidia.inferenceserver.ModelMetadataResponse.inputs', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='outputs', full_name='nvidia.inferenceserver.ModelMetadataResponse.outputs', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELMETADATARESPONSE_TENSORMETADATA, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=433,
  serialized_end=728,
)


_INFERPARAMETER = _descriptor.Descriptor(
  name='InferParameter',
  full_name='nvidia.inferenceserver.InferParameter',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='bool_param', full_name='nvidia.inferenceserver.InferParameter.bool_param', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int64_param', full_name='nvidia.inferenceserver.InferParameter.int64_param', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='string_param', full_name='nvidia.inferenceserver.InferParameter.string_param', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='parameter_choice', full_name='nvidia.inferenceserver.InferParameter.parameter_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=730,
  serialized_end=835,
)


_INFERTENSORCONTENTS = _descriptor.Descriptor(
  name='InferTensorContents',
  full_name='nvidia.inferenceserver.InferTensorContents',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='raw_contents', full_name='nvidia.inferenceserver.InferTensorContents.raw_contents', index=0,
      number=1, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value=_b(""),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='bool_contents', full_name='nvidia.inferenceserver.InferTensorContents.bool_contents', index=1,
      number=2, type=8, cpp_type=7, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int_contents', full_name='nvidia.inferenceserver.InferTensorContents.int_contents', index=2,
      number=3, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int64_contents', full_name='nvidia.inferenceserver.InferTensorContents.int64_contents', index=3,
      number=4, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='uint_contents', full_name='nvidia.inferenceserver.InferTensorContents.uint_contents', index=4,
      number=5, type=13, cpp_type=3, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='uint64_contents', full_name='nvidia.inferenceserver.InferTensorContents.uint64_contents', index=5,
      number=6, type=4, cpp_type=4, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fp32_contents', full_name='nvidia.inferenceserver.InferTensorContents.fp32_contents', index=6,
      number=7, type=2, cpp_type=6, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fp64_contents', full_name='nvidia.inferenceserver.InferTensorContents.fp64_contents', index=7,
      number=8, type=1, cpp_type=5, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='byte_contents', full_name='nvidia.inferenceserver.InferTensorContents.byte_contents', index=8,
      number=9, type=12, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=838,
  serialized_end=1067,
)


_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1615,
  serialized_end=1704,
)

_MODELINFERREQUEST_INFERINPUTTENSOR = _descriptor.Descriptor(
  name='InferInputTensor',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='datatype', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.datatype', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.shape', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.parameters', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='contents', full_name='nvidia.inferenceserver.ModelInferRequest.InferInputTensor.contents', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1389,
  serialized_end=1704,
)

_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1615,
  serialized_end=1704,
)

_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR = _descriptor.Descriptor(
  name='InferRequestedOutputTensor',
  full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.parameters', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1707,
  serialized_end=1946,
)

_MODELINFERREQUEST_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferRequest.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferRequest.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferRequest.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1615,
  serialized_end=1704,
)

_MODELINFERREQUEST = _descriptor.Descriptor(
  name='ModelInferRequest',
  full_name='nvidia.inferenceserver.ModelInferRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.ModelInferRequest.model_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_version', full_name='nvidia.inferenceserver.ModelInferRequest.model_version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='id', full_name='nvidia.inferenceserver.ModelInferRequest.id', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferRequest.parameters', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='inputs', full_name='nvidia.inferenceserver.ModelInferRequest.inputs', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='outputs', full_name='nvidia.inferenceserver.ModelInferRequest.outputs', index=5,
      number=6, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERREQUEST_INFERINPUTTENSOR, _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR, _MODELINFERREQUEST_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1070,
  serialized_end=2037,
)


_MODELINFERRESPONSE_INFEROUTPUTTENSOR = _descriptor.Descriptor(
  name='InferOutputTensor',
  full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='datatype', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.datatype', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.shape', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='contents', full_name='nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.contents', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2277,
  serialized_end=2406,
)

_MODELINFERRESPONSE_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelInferResponse.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelInferResponse.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelInferResponse.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1615,
  serialized_end=1704,
)

_MODELINFERRESPONSE = _descriptor.Descriptor(
  name='ModelInferResponse',
  full_name='nvidia.inferenceserver.ModelInferResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.ModelInferResponse.model_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_version', full_name='nvidia.inferenceserver.ModelInferResponse.model_version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='id', full_name='nvidia.inferenceserver.ModelInferResponse.id', index=2,
      number=3, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelInferResponse.parameters', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='outputs', full_name='nvidia.inferenceserver.ModelInferResponse.outputs', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELINFERRESPONSE_INFEROUTPUTTENSOR, _MODELINFERRESPONSE_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2040,
  serialized_end=2497,
)


_MODELSTREAMINFERRESPONSE = _descriptor.Descriptor(
  name='ModelStreamInferResponse',
  full_name='nvidia.inferenceserver.ModelStreamInferResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='error_message', full_name='nvidia.inferenceserver.ModelStreamInferResponse.error_message', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='infer_response', full_name='nvidia.inferenceserver.ModelStreamInferResponse.infer_response', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2499,
  serialized_end=2616,
)


_MODELCONFIGREQUEST = _descriptor.Descriptor(
  name='ModelConfigRequest',
  full_name='nvidia.inferenceserver.ModelConfigRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelConfigRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelConfigRequest.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2618,
  serialized_end=2669,
)


_MODELCONFIGRESPONSE = _descriptor.Descriptor(
  name='ModelConfigResponse',
  full_name='nvidia.inferenceserver.ModelConfigResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='config', full_name='nvidia.inferenceserver.ModelConfigResponse.config', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2671,
  serialized_end=2745,
)


_MODELSTATISTICSREQUEST = _descriptor.Descriptor(
  name='ModelStatisticsRequest',
  full_name='nvidia.inferenceserver.ModelStatisticsRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelStatisticsRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelStatisticsRequest.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2747,
  serialized_end=2802,
)


_STATISTICDURATION = _descriptor.Descriptor(
  name='StatisticDuration',
  full_name='nvidia.inferenceserver.StatisticDuration',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='count', full_name='nvidia.inferenceserver.StatisticDuration.count', index=0,
      number=1, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='ns', full_name='nvidia.inferenceserver.StatisticDuration.ns', index=1,
      number=2, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2804,
  serialized_end=2850,
)


_INFERSTATISTICS = _descriptor.Descriptor(
  name='InferStatistics',
  full_name='nvidia.inferenceserver.InferStatistics',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='success', full_name='nvidia.inferenceserver.InferStatistics.success', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fail', full_name='nvidia.inferenceserver.InferStatistics.fail', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='queue', full_name='nvidia.inferenceserver.InferStatistics.queue', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='compute_input', full_name='nvidia.inferenceserver.InferStatistics.compute_input', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='compute_infer', full_name='nvidia.inferenceserver.InferStatistics.compute_infer', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='compute_output', full_name='nvidia.inferenceserver.InferStatistics.compute_output', index=5,
      number=6, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2853,
  serialized_end=3244,
)


_MODELSTATISTICS = _descriptor.Descriptor(
  name='ModelStatistics',
  full_name='nvidia.inferenceserver.ModelStatistics',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelStatistics.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version', full_name='nvidia.inferenceserver.ModelStatistics.version', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='inference_stats', full_name='nvidia.inferenceserver.ModelStatistics.inference_stats', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3246,
  serialized_end=3360,
)


_MODELSTATISTICSRESPONSE = _descriptor.Descriptor(
  name='ModelStatisticsResponse',
  full_name='nvidia.inferenceserver.ModelStatisticsResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_stats', full_name='nvidia.inferenceserver.ModelStatisticsResponse.model_stats', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3362,
  serialized_end=3449,
)


_REPOSITORYINDEXREQUEST = _descriptor.Descriptor(
  name='RepositoryIndexRequest',
  full_name='nvidia.inferenceserver.RepositoryIndexRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='repository_name', full_name='nvidia.inferenceserver.RepositoryIndexRequest.repository_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3451,
  serialized_end=3500,
)


_REPOSITORYINDEXRESPONSE_MODELINDEX = _descriptor.Descriptor(
  name='ModelIndex',
  full_name='nvidia.inferenceserver.RepositoryIndexResponse.ModelIndex',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.RepositoryIndexResponse.ModelIndex.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3606,
  serialized_end=3632,
)

_REPOSITORYINDEXRESPONSE = _descriptor.Descriptor(
  name='RepositoryIndexResponse',
  full_name='nvidia.inferenceserver.RepositoryIndexResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='models', full_name='nvidia.inferenceserver.RepositoryIndexResponse.models', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_REPOSITORYINDEXRESPONSE_MODELINDEX, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3503,
  serialized_end=3632,
)


_REPOSITORYMODELLOADREQUEST = _descriptor.Descriptor(
  name='RepositoryModelLoadRequest',
  full_name='nvidia.inferenceserver.RepositoryModelLoadRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='repository_name', full_name='nvidia.inferenceserver.RepositoryModelLoadRequest.repository_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.RepositoryModelLoadRequest.model_name', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3634,
  serialized_end=3707,
)


_REPOSITORYMODELLOADRESPONSE = _descriptor.Descriptor(
  name='RepositoryModelLoadResponse',
  full_name='nvidia.inferenceserver.RepositoryModelLoadResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3709,
  serialized_end=3738,
)


_REPOSITORYMODELUNLOADREQUEST = _descriptor.Descriptor(
  name='RepositoryModelUnloadRequest',
  full_name='nvidia.inferenceserver.RepositoryModelUnloadRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='repository_name', full_name='nvidia.inferenceserver.RepositoryModelUnloadRequest.repository_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.RepositoryModelUnloadRequest.model_name', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3740,
  serialized_end=3815,
)


_REPOSITORYMODELUNLOADRESPONSE = _descriptor.Descriptor(
  name='RepositoryModelUnloadResponse',
  full_name='nvidia.inferenceserver.RepositoryModelUnloadResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3817,
  serialized_end=3848,
)


_SYSTEMSHAREDMEMORYSTATUSREQUEST = _descriptor.Descriptor(
  name='SystemSharedMemoryStatusRequest',
  full_name='nvidia.inferenceserver.SystemSharedMemoryStatusRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3850,
  serialized_end=3897,
)


_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS = _descriptor.Descriptor(
  name='RegionStatus',
  full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus.key', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='offset', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus.offset', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='byte_size', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus.byte_size', index=3,
      number=4, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4024,
  serialized_end=4100,
)

_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY = _descriptor.Descriptor(
  name='RegionsEntry',
  full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionsEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionsEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionsEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4102,
  serialized_end=4219,
)

_SYSTEMSHAREDMEMORYSTATUSRESPONSE = _descriptor.Descriptor(
  name='SystemSharedMemoryStatusResponse',
  full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='regions', full_name='nvidia.inferenceserver.SystemSharedMemoryStatusResponse.regions', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS, _SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3900,
  serialized_end=4219,
)


_SYSTEMSHAREDMEMORYREGISTERREQUEST = _descriptor.Descriptor(
  name='SystemSharedMemoryRegisterRequest',
  full_name='nvidia.inferenceserver.SystemSharedMemoryRegisterRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.SystemSharedMemoryRegisterRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.SystemSharedMemoryRegisterRequest.key', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='offset', full_name='nvidia.inferenceserver.SystemSharedMemoryRegisterRequest.offset', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='byte_size', full_name='nvidia.inferenceserver.SystemSharedMemoryRegisterRequest.byte_size', index=3,
      number=4, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4221,
  serialized_end=4318,
)


_SYSTEMSHAREDMEMORYREGISTERRESPONSE = _descriptor.Descriptor(
  name='SystemSharedMemoryRegisterResponse',
  full_name='nvidia.inferenceserver.SystemSharedMemoryRegisterResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4320,
  serialized_end=4356,
)


_SYSTEMSHAREDMEMORYUNREGISTERREQUEST = _descriptor.Descriptor(
  name='SystemSharedMemoryUnregisterRequest',
  full_name='nvidia.inferenceserver.SystemSharedMemoryUnregisterRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.SystemSharedMemoryUnregisterRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4358,
  serialized_end=4409,
)


_SYSTEMSHAREDMEMORYUNREGISTERRESPONSE = _descriptor.Descriptor(
  name='SystemSharedMemoryUnregisterResponse',
  full_name='nvidia.inferenceserver.SystemSharedMemoryUnregisterResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4411,
  serialized_end=4449,
)


_CUDASHAREDMEMORYSTATUSREQUEST = _descriptor.Descriptor(
  name='CudaSharedMemoryStatusRequest',
  full_name='nvidia.inferenceserver.CudaSharedMemoryStatusRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4451,
  serialized_end=4496,
)


_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS = _descriptor.Descriptor(
  name='RegionStatus',
  full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionStatus',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionStatus.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='device_id', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionStatus.device_id', index=1,
      number=2, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='byte_size', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionStatus.byte_size', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4619,
  serialized_end=4685,
)

_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY = _descriptor.Descriptor(
  name='RegionsEntry',
  full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionsEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionsEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionsEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4687,
  serialized_end=4802,
)

_CUDASHAREDMEMORYSTATUSRESPONSE = _descriptor.Descriptor(
  name='CudaSharedMemoryStatusResponse',
  full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='regions', full_name='nvidia.inferenceserver.CudaSharedMemoryStatusResponse.regions', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS, _CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4499,
  serialized_end=4802,
)


_CUDASHAREDMEMORYREGISTERREQUEST = _descriptor.Descriptor(
  name='CudaSharedMemoryRegisterRequest',
  full_name='nvidia.inferenceserver.CudaSharedMemoryRegisterRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.CudaSharedMemoryRegisterRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='raw_handle', full_name='nvidia.inferenceserver.CudaSharedMemoryRegisterRequest.raw_handle', index=1,
      number=2, type=12, cpp_type=9, label=1,
      has_default_value=False, default_value=_b(""),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='device_id', full_name='nvidia.inferenceserver.CudaSharedMemoryRegisterRequest.device_id', index=2,
      number=3, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='byte_size', full_name='nvidia.inferenceserver.CudaSharedMemoryRegisterRequest.byte_size', index=3,
      number=4, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4804,
  serialized_end=4909,
)


_CUDASHAREDMEMORYREGISTERRESPONSE = _descriptor.Descriptor(
  name='CudaSharedMemoryRegisterResponse',
  full_name='nvidia.inferenceserver.CudaSharedMemoryRegisterResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4911,
  serialized_end=4945,
)


_CUDASHAREDMEMORYUNREGISTERREQUEST = _descriptor.Descriptor(
  name='CudaSharedMemoryUnregisterRequest',
  full_name='nvidia.inferenceserver.CudaSharedMemoryUnregisterRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.CudaSharedMemoryUnregisterRequest.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4947,
  serialized_end=4996,
)


_CUDASHAREDMEMORYUNREGISTERRESPONSE = _descriptor.Descriptor(
  name='CudaSharedMemoryUnregisterResponse',
  full_name='nvidia.inferenceserver.CudaSharedMemoryUnregisterResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4998,
  serialized_end=5034,
)

_MODELMETADATARESPONSE_TENSORMETADATA.containing_type = _MODELMETADATARESPONSE
_MODELMETADATARESPONSE.fields_by_name['inputs'].message_type = _MODELMETADATARESPONSE_TENSORMETADATA
_MODELMETADATARESPONSE.fields_by_name['outputs'].message_type = _MODELMETADATARESPONSE_TENSORMETADATA
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['bool_param'])
_INFERPARAMETER.fields_by_name['bool_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['int64_param'])
_INFERPARAMETER.fields_by_name['int64_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_INFERPARAMETER.oneofs_by_name['parameter_choice'].fields.append(
  _INFERPARAMETER.fields_by_name['string_param'])
_INFERPARAMETER.fields_by_name['string_param'].containing_oneof = _INFERPARAMETER.oneofs_by_name['parameter_choice']
_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY.containing_type = _MODELINFERREQUEST_INFERINPUTTENSOR
_MODELINFERREQUEST_INFERINPUTTENSOR.fields_by_name['parameters'].message_type = _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY
_MODELINFERREQUEST_INFERINPUTTENSOR.fields_by_name['contents'].message_type = _INFERTENSORCONTENTS
_MODELINFERREQUEST_INFERINPUTTENSOR.containing_type = _MODELINFERREQUEST
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY.containing_type = _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR.fields_by_name['parameters'].message_type = _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR.containing_type = _MODELINFERREQUEST
_MODELINFERREQUEST_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERREQUEST_PARAMETERSENTRY.containing_type = _MODELINFERREQUEST
_MODELINFERREQUEST.fields_by_name['parameters'].message_type = _MODELINFERREQUEST_PARAMETERSENTRY
_MODELINFERREQUEST.fields_by_name['inputs'].message_type = _MODELINFERREQUEST_INFERINPUTTENSOR
_MODELINFERREQUEST.fields_by_name['outputs'].message_type = _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR
_MODELINFERRESPONSE_INFEROUTPUTTENSOR.fields_by_name['contents'].message_type = _INFERTENSORCONTENTS
_MODELINFERRESPONSE_INFEROUTPUTTENSOR.containing_type = _MODELINFERRESPONSE
_MODELINFERRESPONSE_PARAMETERSENTRY.fields_by_name['value'].message_type = _INFERPARAMETER
_MODELINFERRESPONSE_PARAMETERSENTRY.containing_type = _MODELINFERRESPONSE
_MODELINFERRESPONSE.fields_by_name['parameters'].message_type = _MODELINFERRESPONSE_PARAMETERSENTRY
_MODELINFERRESPONSE.fields_by_name['outputs'].message_type = _MODELINFERRESPONSE_INFEROUTPUTTENSOR
_MODELSTREAMINFERRESPONSE.fields_by_name['infer_response'].message_type = _MODELINFERRESPONSE
_MODELCONFIGRESPONSE.fields_by_name['config'].message_type = model__config__pb2._MODELCONFIG
_INFERSTATISTICS.fields_by_name['success'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['fail'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['queue'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['compute_input'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['compute_infer'].message_type = _STATISTICDURATION
_INFERSTATISTICS.fields_by_name['compute_output'].message_type = _STATISTICDURATION
_MODELSTATISTICS.fields_by_name['inference_stats'].message_type = _INFERSTATISTICS
_MODELSTATISTICSRESPONSE.fields_by_name['model_stats'].message_type = _MODELSTATISTICS
_REPOSITORYINDEXRESPONSE_MODELINDEX.containing_type = _REPOSITORYINDEXRESPONSE
_REPOSITORYINDEXRESPONSE.fields_by_name['models'].message_type = _REPOSITORYINDEXRESPONSE_MODELINDEX
_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS.containing_type = _SYSTEMSHAREDMEMORYSTATUSRESPONSE
_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY.fields_by_name['value'].message_type = _SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS
_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY.containing_type = _SYSTEMSHAREDMEMORYSTATUSRESPONSE
_SYSTEMSHAREDMEMORYSTATUSRESPONSE.fields_by_name['regions'].message_type = _SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY
_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS.containing_type = _CUDASHAREDMEMORYSTATUSRESPONSE
_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY.fields_by_name['value'].message_type = _CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS
_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY.containing_type = _CUDASHAREDMEMORYSTATUSRESPONSE
_CUDASHAREDMEMORYSTATUSRESPONSE.fields_by_name['regions'].message_type = _CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY
DESCRIPTOR.message_types_by_name['ServerLiveRequest'] = _SERVERLIVEREQUEST
DESCRIPTOR.message_types_by_name['ServerLiveResponse'] = _SERVERLIVERESPONSE
DESCRIPTOR.message_types_by_name['ServerReadyRequest'] = _SERVERREADYREQUEST
DESCRIPTOR.message_types_by_name['ServerReadyResponse'] = _SERVERREADYRESPONSE
DESCRIPTOR.message_types_by_name['ModelReadyRequest'] = _MODELREADYREQUEST
DESCRIPTOR.message_types_by_name['ModelReadyResponse'] = _MODELREADYRESPONSE
DESCRIPTOR.message_types_by_name['ServerMetadataRequest'] = _SERVERMETADATAREQUEST
DESCRIPTOR.message_types_by_name['ServerMetadataResponse'] = _SERVERMETADATARESPONSE
DESCRIPTOR.message_types_by_name['ModelMetadataRequest'] = _MODELMETADATAREQUEST
DESCRIPTOR.message_types_by_name['ModelMetadataResponse'] = _MODELMETADATARESPONSE
DESCRIPTOR.message_types_by_name['InferParameter'] = _INFERPARAMETER
DESCRIPTOR.message_types_by_name['InferTensorContents'] = _INFERTENSORCONTENTS
DESCRIPTOR.message_types_by_name['ModelInferRequest'] = _MODELINFERREQUEST
DESCRIPTOR.message_types_by_name['ModelInferResponse'] = _MODELINFERRESPONSE
DESCRIPTOR.message_types_by_name['ModelStreamInferResponse'] = _MODELSTREAMINFERRESPONSE
DESCRIPTOR.message_types_by_name['ModelConfigRequest'] = _MODELCONFIGREQUEST
DESCRIPTOR.message_types_by_name['ModelConfigResponse'] = _MODELCONFIGRESPONSE
DESCRIPTOR.message_types_by_name['ModelStatisticsRequest'] = _MODELSTATISTICSREQUEST
DESCRIPTOR.message_types_by_name['StatisticDuration'] = _STATISTICDURATION
DESCRIPTOR.message_types_by_name['InferStatistics'] = _INFERSTATISTICS
DESCRIPTOR.message_types_by_name['ModelStatistics'] = _MODELSTATISTICS
DESCRIPTOR.message_types_by_name['ModelStatisticsResponse'] = _MODELSTATISTICSRESPONSE
DESCRIPTOR.message_types_by_name['RepositoryIndexRequest'] = _REPOSITORYINDEXREQUEST
DESCRIPTOR.message_types_by_name['RepositoryIndexResponse'] = _REPOSITORYINDEXRESPONSE
DESCRIPTOR.message_types_by_name['RepositoryModelLoadRequest'] = _REPOSITORYMODELLOADREQUEST
DESCRIPTOR.message_types_by_name['RepositoryModelLoadResponse'] = _REPOSITORYMODELLOADRESPONSE
DESCRIPTOR.message_types_by_name['RepositoryModelUnloadRequest'] = _REPOSITORYMODELUNLOADREQUEST
DESCRIPTOR.message_types_by_name['RepositoryModelUnloadResponse'] = _REPOSITORYMODELUNLOADRESPONSE
DESCRIPTOR.message_types_by_name['SystemSharedMemoryStatusRequest'] = _SYSTEMSHAREDMEMORYSTATUSREQUEST
DESCRIPTOR.message_types_by_name['SystemSharedMemoryStatusResponse'] = _SYSTEMSHAREDMEMORYSTATUSRESPONSE
DESCRIPTOR.message_types_by_name['SystemSharedMemoryRegisterRequest'] = _SYSTEMSHAREDMEMORYREGISTERREQUEST
DESCRIPTOR.message_types_by_name['SystemSharedMemoryRegisterResponse'] = _SYSTEMSHAREDMEMORYREGISTERRESPONSE
DESCRIPTOR.message_types_by_name['SystemSharedMemoryUnregisterRequest'] = _SYSTEMSHAREDMEMORYUNREGISTERREQUEST
DESCRIPTOR.message_types_by_name['SystemSharedMemoryUnregisterResponse'] = _SYSTEMSHAREDMEMORYUNREGISTERRESPONSE
DESCRIPTOR.message_types_by_name['CudaSharedMemoryStatusRequest'] = _CUDASHAREDMEMORYSTATUSREQUEST
DESCRIPTOR.message_types_by_name['CudaSharedMemoryStatusResponse'] = _CUDASHAREDMEMORYSTATUSRESPONSE
DESCRIPTOR.message_types_by_name['CudaSharedMemoryRegisterRequest'] = _CUDASHAREDMEMORYREGISTERREQUEST
DESCRIPTOR.message_types_by_name['CudaSharedMemoryRegisterResponse'] = _CUDASHAREDMEMORYREGISTERRESPONSE
DESCRIPTOR.message_types_by_name['CudaSharedMemoryUnregisterRequest'] = _CUDASHAREDMEMORYUNREGISTERREQUEST
DESCRIPTOR.message_types_by_name['CudaSharedMemoryUnregisterResponse'] = _CUDASHAREDMEMORYUNREGISTERRESPONSE
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

ServerLiveRequest = _reflection.GeneratedProtocolMessageType('ServerLiveRequest', (_message.Message,), {
  'DESCRIPTOR' : _SERVERLIVEREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerLiveRequest)
  })
_sym_db.RegisterMessage(ServerLiveRequest)

ServerLiveResponse = _reflection.GeneratedProtocolMessageType('ServerLiveResponse', (_message.Message,), {
  'DESCRIPTOR' : _SERVERLIVERESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerLiveResponse)
  })
_sym_db.RegisterMessage(ServerLiveResponse)

ServerReadyRequest = _reflection.GeneratedProtocolMessageType('ServerReadyRequest', (_message.Message,), {
  'DESCRIPTOR' : _SERVERREADYREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerReadyRequest)
  })
_sym_db.RegisterMessage(ServerReadyRequest)

ServerReadyResponse = _reflection.GeneratedProtocolMessageType('ServerReadyResponse', (_message.Message,), {
  'DESCRIPTOR' : _SERVERREADYRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerReadyResponse)
  })
_sym_db.RegisterMessage(ServerReadyResponse)

ModelReadyRequest = _reflection.GeneratedProtocolMessageType('ModelReadyRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELREADYREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelReadyRequest)
  })
_sym_db.RegisterMessage(ModelReadyRequest)

ModelReadyResponse = _reflection.GeneratedProtocolMessageType('ModelReadyResponse', (_message.Message,), {
  'DESCRIPTOR' : _MODELREADYRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelReadyResponse)
  })
_sym_db.RegisterMessage(ModelReadyResponse)

ServerMetadataRequest = _reflection.GeneratedProtocolMessageType('ServerMetadataRequest', (_message.Message,), {
  'DESCRIPTOR' : _SERVERMETADATAREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerMetadataRequest)
  })
_sym_db.RegisterMessage(ServerMetadataRequest)

ServerMetadataResponse = _reflection.GeneratedProtocolMessageType('ServerMetadataResponse', (_message.Message,), {
  'DESCRIPTOR' : _SERVERMETADATARESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ServerMetadataResponse)
  })
_sym_db.RegisterMessage(ServerMetadataResponse)

ModelMetadataRequest = _reflection.GeneratedProtocolMessageType('ModelMetadataRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELMETADATAREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelMetadataRequest)
  })
_sym_db.RegisterMessage(ModelMetadataRequest)

ModelMetadataResponse = _reflection.GeneratedProtocolMessageType('ModelMetadataResponse', (_message.Message,), {

  'TensorMetadata' : _reflection.GeneratedProtocolMessageType('TensorMetadata', (_message.Message,), {
    'DESCRIPTOR' : _MODELMETADATARESPONSE_TENSORMETADATA,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelMetadataResponse.TensorMetadata)
    })
  ,
  'DESCRIPTOR' : _MODELMETADATARESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelMetadataResponse)
  })
_sym_db.RegisterMessage(ModelMetadataResponse)
_sym_db.RegisterMessage(ModelMetadataResponse.TensorMetadata)

InferParameter = _reflection.GeneratedProtocolMessageType('InferParameter', (_message.Message,), {
  'DESCRIPTOR' : _INFERPARAMETER,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferParameter)
  })
_sym_db.RegisterMessage(InferParameter)

InferTensorContents = _reflection.GeneratedProtocolMessageType('InferTensorContents', (_message.Message,), {
  'DESCRIPTOR' : _INFERTENSORCONTENTS,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferTensorContents)
  })
_sym_db.RegisterMessage(InferTensorContents)

ModelInferRequest = _reflection.GeneratedProtocolMessageType('ModelInferRequest', (_message.Message,), {

  'InferInputTensor' : _reflection.GeneratedProtocolMessageType('InferInputTensor', (_message.Message,), {

    'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
      'DESCRIPTOR' : _MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY,
      '__module__' : 'grpc_service_v2_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferInputTensor.ParametersEntry)
      })
    ,
    'DESCRIPTOR' : _MODELINFERREQUEST_INFERINPUTTENSOR,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferInputTensor)
    })
  ,

  'InferRequestedOutputTensor' : _reflection.GeneratedProtocolMessageType('InferRequestedOutputTensor', (_message.Message,), {

    'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
      'DESCRIPTOR' : _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY,
      '__module__' : 'grpc_service_v2_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry)
      })
    ,
    'DESCRIPTOR' : _MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.InferRequestedOutputTensor)
    })
  ,

  'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
    'DESCRIPTOR' : _MODELINFERREQUEST_PARAMETERSENTRY,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest.ParametersEntry)
    })
  ,
  'DESCRIPTOR' : _MODELINFERREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferRequest)
  })
_sym_db.RegisterMessage(ModelInferRequest)
_sym_db.RegisterMessage(ModelInferRequest.InferInputTensor)
_sym_db.RegisterMessage(ModelInferRequest.InferInputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferRequest.InferRequestedOutputTensor)
_sym_db.RegisterMessage(ModelInferRequest.InferRequestedOutputTensor.ParametersEntry)
_sym_db.RegisterMessage(ModelInferRequest.ParametersEntry)

ModelInferResponse = _reflection.GeneratedProtocolMessageType('ModelInferResponse', (_message.Message,), {

  'InferOutputTensor' : _reflection.GeneratedProtocolMessageType('InferOutputTensor', (_message.Message,), {
    'DESCRIPTOR' : _MODELINFERRESPONSE_INFEROUTPUTTENSOR,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferResponse.InferOutputTensor)
    })
  ,

  'ParametersEntry' : _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), {
    'DESCRIPTOR' : _MODELINFERRESPONSE_PARAMETERSENTRY,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferResponse.ParametersEntry)
    })
  ,
  'DESCRIPTOR' : _MODELINFERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInferResponse)
  })
_sym_db.RegisterMessage(ModelInferResponse)
_sym_db.RegisterMessage(ModelInferResponse.InferOutputTensor)
_sym_db.RegisterMessage(ModelInferResponse.ParametersEntry)

ModelStreamInferResponse = _reflection.GeneratedProtocolMessageType('ModelStreamInferResponse', (_message.Message,), {
  'DESCRIPTOR' : _MODELSTREAMINFERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelStreamInferResponse)
  })
_sym_db.RegisterMessage(ModelStreamInferResponse)

ModelConfigRequest = _reflection.GeneratedProtocolMessageType('ModelConfigRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELCONFIGREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfigRequest)
  })
_sym_db.RegisterMessage(ModelConfigRequest)

ModelConfigResponse = _reflection.GeneratedProtocolMessageType('ModelConfigResponse', (_message.Message,), {
  'DESCRIPTOR' : _MODELCONFIGRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfigResponse)
  })
_sym_db.RegisterMessage(ModelConfigResponse)

ModelStatisticsRequest = _reflection.GeneratedProtocolMessageType('ModelStatisticsRequest', (_message.Message,), {
  'DESCRIPTOR' : _MODELSTATISTICSREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelStatisticsRequest)
  })
_sym_db.RegisterMessage(ModelStatisticsRequest)

StatisticDuration = _reflection.GeneratedProtocolMessageType('StatisticDuration', (_message.Message,), {
  'DESCRIPTOR' : _STATISTICDURATION,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.StatisticDuration)
  })
_sym_db.RegisterMessage(StatisticDuration)

InferStatistics = _reflection.GeneratedProtocolMessageType('InferStatistics', (_message.Message,), {
  'DESCRIPTOR' : _INFERSTATISTICS,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.InferStatistics)
  })
_sym_db.RegisterMessage(InferStatistics)

ModelStatistics = _reflection.GeneratedProtocolMessageType('ModelStatistics', (_message.Message,), {
  'DESCRIPTOR' : _MODELSTATISTICS,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelStatistics)
  })
_sym_db.RegisterMessage(ModelStatistics)

ModelStatisticsResponse = _reflection.GeneratedProtocolMessageType('ModelStatisticsResponse', (_message.Message,), {
  'DESCRIPTOR' : _MODELSTATISTICSRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelStatisticsResponse)
  })
_sym_db.RegisterMessage(ModelStatisticsResponse)

RepositoryIndexRequest = _reflection.GeneratedProtocolMessageType('RepositoryIndexRequest', (_message.Message,), {
  'DESCRIPTOR' : _REPOSITORYINDEXREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryIndexRequest)
  })
_sym_db.RegisterMessage(RepositoryIndexRequest)

RepositoryIndexResponse = _reflection.GeneratedProtocolMessageType('RepositoryIndexResponse', (_message.Message,), {

  'ModelIndex' : _reflection.GeneratedProtocolMessageType('ModelIndex', (_message.Message,), {
    'DESCRIPTOR' : _REPOSITORYINDEXRESPONSE_MODELINDEX,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryIndexResponse.ModelIndex)
    })
  ,
  'DESCRIPTOR' : _REPOSITORYINDEXRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryIndexResponse)
  })
_sym_db.RegisterMessage(RepositoryIndexResponse)
_sym_db.RegisterMessage(RepositoryIndexResponse.ModelIndex)

RepositoryModelLoadRequest = _reflection.GeneratedProtocolMessageType('RepositoryModelLoadRequest', (_message.Message,), {
  'DESCRIPTOR' : _REPOSITORYMODELLOADREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryModelLoadRequest)
  })
_sym_db.RegisterMessage(RepositoryModelLoadRequest)

RepositoryModelLoadResponse = _reflection.GeneratedProtocolMessageType('RepositoryModelLoadResponse', (_message.Message,), {
  'DESCRIPTOR' : _REPOSITORYMODELLOADRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryModelLoadResponse)
  })
_sym_db.RegisterMessage(RepositoryModelLoadResponse)

RepositoryModelUnloadRequest = _reflection.GeneratedProtocolMessageType('RepositoryModelUnloadRequest', (_message.Message,), {
  'DESCRIPTOR' : _REPOSITORYMODELUNLOADREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryModelUnloadRequest)
  })
_sym_db.RegisterMessage(RepositoryModelUnloadRequest)

RepositoryModelUnloadResponse = _reflection.GeneratedProtocolMessageType('RepositoryModelUnloadResponse', (_message.Message,), {
  'DESCRIPTOR' : _REPOSITORYMODELUNLOADRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.RepositoryModelUnloadResponse)
  })
_sym_db.RegisterMessage(RepositoryModelUnloadResponse)

SystemSharedMemoryStatusRequest = _reflection.GeneratedProtocolMessageType('SystemSharedMemoryStatusRequest', (_message.Message,), {
  'DESCRIPTOR' : _SYSTEMSHAREDMEMORYSTATUSREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryStatusRequest)
  })
_sym_db.RegisterMessage(SystemSharedMemoryStatusRequest)

SystemSharedMemoryStatusResponse = _reflection.GeneratedProtocolMessageType('SystemSharedMemoryStatusResponse', (_message.Message,), {

  'RegionStatus' : _reflection.GeneratedProtocolMessageType('RegionStatus', (_message.Message,), {
    'DESCRIPTOR' : _SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionStatus)
    })
  ,

  'RegionsEntry' : _reflection.GeneratedProtocolMessageType('RegionsEntry', (_message.Message,), {
    'DESCRIPTOR' : _SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryStatusResponse.RegionsEntry)
    })
  ,
  'DESCRIPTOR' : _SYSTEMSHAREDMEMORYSTATUSRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryStatusResponse)
  })
_sym_db.RegisterMessage(SystemSharedMemoryStatusResponse)
_sym_db.RegisterMessage(SystemSharedMemoryStatusResponse.RegionStatus)
_sym_db.RegisterMessage(SystemSharedMemoryStatusResponse.RegionsEntry)

SystemSharedMemoryRegisterRequest = _reflection.GeneratedProtocolMessageType('SystemSharedMemoryRegisterRequest', (_message.Message,), {
  'DESCRIPTOR' : _SYSTEMSHAREDMEMORYREGISTERREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryRegisterRequest)
  })
_sym_db.RegisterMessage(SystemSharedMemoryRegisterRequest)

SystemSharedMemoryRegisterResponse = _reflection.GeneratedProtocolMessageType('SystemSharedMemoryRegisterResponse', (_message.Message,), {
  'DESCRIPTOR' : _SYSTEMSHAREDMEMORYREGISTERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryRegisterResponse)
  })
_sym_db.RegisterMessage(SystemSharedMemoryRegisterResponse)

SystemSharedMemoryUnregisterRequest = _reflection.GeneratedProtocolMessageType('SystemSharedMemoryUnregisterRequest', (_message.Message,), {
  'DESCRIPTOR' : _SYSTEMSHAREDMEMORYUNREGISTERREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryUnregisterRequest)
  })
_sym_db.RegisterMessage(SystemSharedMemoryUnregisterRequest)

SystemSharedMemoryUnregisterResponse = _reflection.GeneratedProtocolMessageType('SystemSharedMemoryUnregisterResponse', (_message.Message,), {
  'DESCRIPTOR' : _SYSTEMSHAREDMEMORYUNREGISTERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.SystemSharedMemoryUnregisterResponse)
  })
_sym_db.RegisterMessage(SystemSharedMemoryUnregisterResponse)

CudaSharedMemoryStatusRequest = _reflection.GeneratedProtocolMessageType('CudaSharedMemoryStatusRequest', (_message.Message,), {
  'DESCRIPTOR' : _CUDASHAREDMEMORYSTATUSREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryStatusRequest)
  })
_sym_db.RegisterMessage(CudaSharedMemoryStatusRequest)

CudaSharedMemoryStatusResponse = _reflection.GeneratedProtocolMessageType('CudaSharedMemoryStatusResponse', (_message.Message,), {

  'RegionStatus' : _reflection.GeneratedProtocolMessageType('RegionStatus', (_message.Message,), {
    'DESCRIPTOR' : _CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSTATUS,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionStatus)
    })
  ,

  'RegionsEntry' : _reflection.GeneratedProtocolMessageType('RegionsEntry', (_message.Message,), {
    'DESCRIPTOR' : _CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY,
    '__module__' : 'grpc_service_v2_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryStatusResponse.RegionsEntry)
    })
  ,
  'DESCRIPTOR' : _CUDASHAREDMEMORYSTATUSRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryStatusResponse)
  })
_sym_db.RegisterMessage(CudaSharedMemoryStatusResponse)
_sym_db.RegisterMessage(CudaSharedMemoryStatusResponse.RegionStatus)
_sym_db.RegisterMessage(CudaSharedMemoryStatusResponse.RegionsEntry)

CudaSharedMemoryRegisterRequest = _reflection.GeneratedProtocolMessageType('CudaSharedMemoryRegisterRequest', (_message.Message,), {
  'DESCRIPTOR' : _CUDASHAREDMEMORYREGISTERREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryRegisterRequest)
  })
_sym_db.RegisterMessage(CudaSharedMemoryRegisterRequest)

CudaSharedMemoryRegisterResponse = _reflection.GeneratedProtocolMessageType('CudaSharedMemoryRegisterResponse', (_message.Message,), {
  'DESCRIPTOR' : _CUDASHAREDMEMORYREGISTERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryRegisterResponse)
  })
_sym_db.RegisterMessage(CudaSharedMemoryRegisterResponse)

CudaSharedMemoryUnregisterRequest = _reflection.GeneratedProtocolMessageType('CudaSharedMemoryUnregisterRequest', (_message.Message,), {
  'DESCRIPTOR' : _CUDASHAREDMEMORYUNREGISTERREQUEST,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryUnregisterRequest)
  })
_sym_db.RegisterMessage(CudaSharedMemoryUnregisterRequest)

CudaSharedMemoryUnregisterResponse = _reflection.GeneratedProtocolMessageType('CudaSharedMemoryUnregisterResponse', (_message.Message,), {
  'DESCRIPTOR' : _CUDASHAREDMEMORYUNREGISTERRESPONSE,
  '__module__' : 'grpc_service_v2_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.CudaSharedMemoryUnregisterResponse)
  })
_sym_db.RegisterMessage(CudaSharedMemoryUnregisterResponse)


_MODELINFERREQUEST_INFERINPUTTENSOR_PARAMETERSENTRY._options = None
_MODELINFERREQUEST_INFERREQUESTEDOUTPUTTENSOR_PARAMETERSENTRY._options = None
_MODELINFERREQUEST_PARAMETERSENTRY._options = None
_MODELINFERRESPONSE_PARAMETERSENTRY._options = None
_SYSTEMSHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY._options = None
_CUDASHAREDMEMORYSTATUSRESPONSE_REGIONSENTRY._options = None

_GRPCINFERENCESERVICE = _descriptor.ServiceDescriptor(
  name='GRPCInferenceService',
  full_name='nvidia.inferenceserver.GRPCInferenceService',
  file=DESCRIPTOR,
  index=0,
  serialized_options=None,
  serialized_start=5037,
  serialized_end=7324,
  methods=[
  _descriptor.MethodDescriptor(
    name='ServerLive',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ServerLive',
    index=0,
    containing_service=None,
    input_type=_SERVERLIVEREQUEST,
    output_type=_SERVERLIVERESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ServerReady',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ServerReady',
    index=1,
    containing_service=None,
    input_type=_SERVERREADYREQUEST,
    output_type=_SERVERREADYRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelReady',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelReady',
    index=2,
    containing_service=None,
    input_type=_MODELREADYREQUEST,
    output_type=_MODELREADYRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ServerMetadata',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ServerMetadata',
    index=3,
    containing_service=None,
    input_type=_SERVERMETADATAREQUEST,
    output_type=_SERVERMETADATARESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelMetadata',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelMetadata',
    index=4,
    containing_service=None,
    input_type=_MODELMETADATAREQUEST,
    output_type=_MODELMETADATARESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelInfer',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelInfer',
    index=5,
    containing_service=None,
    input_type=_MODELINFERREQUEST,
    output_type=_MODELINFERRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelStreamInfer',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelStreamInfer',
    index=6,
    containing_service=None,
    input_type=_MODELINFERREQUEST,
    output_type=_MODELSTREAMINFERRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelConfig',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelConfig',
    index=7,
    containing_service=None,
    input_type=_MODELCONFIGREQUEST,
    output_type=_MODELCONFIGRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='ModelStatistics',
    full_name='nvidia.inferenceserver.GRPCInferenceService.ModelStatistics',
    index=8,
    containing_service=None,
    input_type=_MODELSTATISTICSREQUEST,
    output_type=_MODELSTATISTICSRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='RepositoryIndex',
    full_name='nvidia.inferenceserver.GRPCInferenceService.RepositoryIndex',
    index=9,
    containing_service=None,
    input_type=_REPOSITORYINDEXREQUEST,
    output_type=_REPOSITORYINDEXRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='RepositoryModelLoad',
    full_name='nvidia.inferenceserver.GRPCInferenceService.RepositoryModelLoad',
    index=10,
    containing_service=None,
    input_type=_REPOSITORYMODELLOADREQUEST,
    output_type=_REPOSITORYMODELLOADRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='RepositoryModelUnload',
    full_name='nvidia.inferenceserver.GRPCInferenceService.RepositoryModelUnload',
    index=11,
    containing_service=None,
    input_type=_REPOSITORYMODELUNLOADREQUEST,
    output_type=_REPOSITORYMODELUNLOADRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='SystemSharedMemoryStatus',
    full_name='nvidia.inferenceserver.GRPCInferenceService.SystemSharedMemoryStatus',
    index=12,
    containing_service=None,
    input_type=_SYSTEMSHAREDMEMORYSTATUSREQUEST,
    output_type=_SYSTEMSHAREDMEMORYSTATUSRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='SystemSharedMemoryRegister',
    full_name='nvidia.inferenceserver.GRPCInferenceService.SystemSharedMemoryRegister',
    index=13,
    containing_service=None,
    input_type=_SYSTEMSHAREDMEMORYREGISTERREQUEST,
    output_type=_SYSTEMSHAREDMEMORYREGISTERRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='SystemSharedMemoryUnregister',
    full_name='nvidia.inferenceserver.GRPCInferenceService.SystemSharedMemoryUnregister',
    index=14,
    containing_service=None,
    input_type=_SYSTEMSHAREDMEMORYUNREGISTERREQUEST,
    output_type=_SYSTEMSHAREDMEMORYUNREGISTERRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='CudaSharedMemoryStatus',
    full_name='nvidia.inferenceserver.GRPCInferenceService.CudaSharedMemoryStatus',
    index=15,
    containing_service=None,
    input_type=_CUDASHAREDMEMORYSTATUSREQUEST,
    output_type=_CUDASHAREDMEMORYSTATUSRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='CudaSharedMemoryRegister',
    full_name='nvidia.inferenceserver.GRPCInferenceService.CudaSharedMemoryRegister',
    index=16,
    containing_service=None,
    input_type=_CUDASHAREDMEMORYREGISTERREQUEST,
    output_type=_CUDASHAREDMEMORYREGISTERRESPONSE,
    serialized_options=None,
  ),
  _descriptor.MethodDescriptor(
    name='CudaSharedMemoryUnregister',
    full_name='nvidia.inferenceserver.GRPCInferenceService.CudaSharedMemoryUnregister',
    index=17,
    containing_service=None,
    input_type=_CUDASHAREDMEMORYUNREGISTERREQUEST,
    output_type=_CUDASHAREDMEMORYUNREGISTERRESPONSE,
    serialized_options=None,
  ),
])
_sym_db.RegisterServiceDescriptor(_GRPCINFERENCESERVICE)

DESCRIPTOR.services_by_name['GRPCInferenceService'] = _GRPCINFERENCESERVICE

# @@protoc_insertion_point(module_scope)
